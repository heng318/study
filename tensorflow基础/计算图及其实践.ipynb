{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算图概念\n",
    "\n",
    "在执行计算，例如训练一个神经网络或计算两个整数的总和，TensorFlow在内部使用数据流图（或计算图）来表示整个计算流程。\n",
    "\n",
    "这是一个由以成分下组成的有向图：\n",
    "\n",
    "![img](images/computational-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ 一组节点，每一个节点代表一个操作\n",
    "+ 一组定向边，每一个边代表执行操作的数据流转路径"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TensorFlow有两种类型的边缘：\n",
    "\n",
    "- 正常：它们只是节点之间的数据结构的载体。一个操作的输出（来自一个节点）成为另一个操作的输入。连接两个节点的边传输与这些值。\n",
    "\n",
    "\n",
    "- 特殊：这个边缘不传递值。它表示两个节点A和B之间的控制依赖性。这意味着节点B只有在A中的操作将在数据操作之间的关系之前结束时才会被执行。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么使用计算图\n",
    "\n",
    "TensorFlow中的一个关键思想是延迟执行，在计算图的构建阶段，您可以编写非常复杂的表达式（我们说它是高度复杂的），\n",
    "\n",
    "但是这些代码只是定义了计算过程, 但是并没有真正的计算。当您想计算他们时，TensorFlow会以最有效的方式进行计算\n",
    "\n",
    "（例如，使用GPU并行执行代码的独立部分）。这样，如果必须处理包含大量节点和图层的复杂模型，图形有助于分配计算负载。\n",
    "\n",
    "同时, 计算过程可以方便的可视化, 并且便于我们理解."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 为什么使用计算图\n",
    "\n",
    "TensorFlow中的一个关键思想是延迟执行，在计算图的构建阶段，您可以编写非常复杂的表达式（我们说它是高度复杂的），\n",
    "\n",
    "但是这些代码只是定义了计算过程, 但是并没有真正的计算。当您想计算他们时，TensorFlow会以最有效的方式进行计算\n",
    "\n",
    "（例如，使用GPU并行执行代码的独立部分）。这样，如果必须处理包含大量节点和图层的复杂模型，图形有助于分配计算负载。\n",
    "\n",
    "同时, 计算过程可以方便的可视化, 并且便于我们理解.\n",
    "\n",
    "## 代码实现简单的计算图\n",
    "\n",
    "如上图所示, 我们可以使用tensorflow实现一个这样的计算图。新手使用tensorflow时, 一定要分清楚两个阶段:\n",
    "\n",
    "图定义阶段和计算阶段。在图定义阶段, 只是定义了计算图, 但并不进行计算; 在计算阶段才会进行计算。\n",
    "\n",
    "## 定义计算图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "a = tf.placeholder(tf.float32, name='a')\n",
    "b = tf.placeholder(tf.float32, name='b')\n",
    "c = a + b\n",
    "d = b + 1\n",
    "e = c * d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 计算\n",
    "计算阶段只有两行代码, 我们需要创建一个会话, 并执行计算图。因为这是在Jupyter notebook环境中, \n",
    "\n",
    "我们使用tf.InteractiveSession更方便。\n",
    "\n",
    "eval方法就是用来执行计算并返回计算结果的。注意因为我们的计算图中定义了两个占位符(placeholder),\n",
    "\n",
    "顾名思义, 他们并没有值, 需要我们在计算的时候传入值, 所以我们使用feed_dict参数来传入值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.InteractiveSession()\n",
    "e.eval(feed_dict={a:1, b:2})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络可以看作是计算图\n",
    "\n",
    "我们先来看下之前文章中用到的一个神经网络图:\n",
    "\n",
    "![title](images/nn.png)\n",
    "\n",
    "我们可以用计算图的形式来表示它, 就是这样:\n",
    "\n",
    "![title](images/nn-graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里L3表示输出层，L2表示隐藏层，L1表示输入层。类似地，θ2表示层2与层3之间的权重矢量; \n",
    "\n",
    "θ1表示层1和层2之间的权向量.σ表示在这些节点内发生的激活函数（这些节点的输出将使用L表示来表示，\n",
    "\n",
    "即L1，L2 ，和L3）。在这种表示中，每个节点不代表单个神经元，而代表一个操作，而箭头不是连接，\n",
    "\n",
    "而是代表沿网络的信息流。这个计算图向我们显示了每一步的操作，以及这些操作运行的输入和输出。\n",
    "\n",
    "L2层在两个输入上运行：输出L1层（向量）和权重θ1，而L3在L2和θ2上运行，并且是我们的最终输出。\n",
    "\n",
    "## 代码实现神经网络的计算图\n",
    "\n",
    "你会看到, 由于计算图的引用, 我们更容易用代码实现这个神经网络结构。\n",
    "\n",
    "## 定义计算图\n",
    "\n",
    "你可能还不清楚tf.Variable和tf.placeholder的用法, 前者用于代表可变量, 一般就是函数的参数,\n",
    "\n",
    "后者用于代表数据, 比如训练数据。后面我们会详细解释这些函数的用法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = tf.placeholder(tf.float64, shape=[3, 1], name= \"X\")\n",
    "theta1 = tf.Variable(np.random.normal(size=(4,3)), name=\"theta1\")\n",
    "L2 = tf.sigmoid(tf.matmul(theta1, X))\n",
    "theta2 = tf.Variable(np.random.normal(size=(2,4)), name=\"theta1\")\n",
    "L3 = tf.sigmoid(tf.matmul(theta2, L2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 执行计算\n",
    "\n",
    "+ 首先因为我们定义了一些Variable, 所以需要先初始化他们, 调用init.run()\n",
    "\n",
    "+ 然后因为定义了一个placeholder, 所以需要在eval的时候, 传入feed_dict参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.95404648],\n",
       "       [0.91124436]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "init.run()\n",
    "x_value = np.array([1,2,3]).reshape([3,1])\n",
    "L3.eval(feed_dict={X:x_value})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
