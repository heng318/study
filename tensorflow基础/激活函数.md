### 一 sigmoid函数
![](http://latex.codecogs.com/gif.latex?S(x)=\frac{1}{1+e^x})

![](https://img-blog.csdnimg.cn/20190426183305644.png)
####  优点
1、它能够把输入的连续实值变换为0和1之间的输出，单调连续，输出范围有限，优化稳定，可作为输出层

2、易求导
####  缺点
1、在深度神经网络中梯度反向传递时导致梯度爆炸和梯度消失

2、输出不是0均值

3、其解析式中含有幂运算，计算机求解时相对来讲比较耗时

### 二 Tanh函数
![](http://latex.codecogs.com/gif.latex?tanh(x)=\frac{1-e^-2x}{1+e^-2x})

 ![](https://img-blog.csdnimg.cn/20190426183849436.png)
 #### 优点：
 1、Tanh函数的输出均值是0，收敛速度比Sigmoid函数快
 
 2、迭代次数相对较少
 #### 缺点
 1、具有软饱和性
 
 2、由软饱和性产生的梯度消失
 ### 三 ReLU函数
 ![](http://latex.codecogs.com/gif.latex?f(x)=max(0,x))
 
 ![](https://img-blog.csdnimg.cn/20190426184342555.png)
### Leaky ReLU函数（PReLU）
![](http://latex.codecogs.com/gif.latex?f(x)=max(αx,x))

![](https://img-blog.csdnimg.cn/20190426184529583.png)
#### 优点
1、ReLU能够快速收敛

2、实现简单

3、有效缓解梯度消失

4、无监督也有很好的表现
#### 缺点
1、随着训练的进行，可能出现神经元死亡、权重无法更新

2、偏移和神经元的死亡会大大影响收敛性
### 四 softplus函数
![](http://latex.codecogs.com/gif.latex?softplus=log(1+e^x))

![](https://img-blog.csdnimg.cn/20190426185653226.png)
#### 优点
1、进行指数运算，计算量少

2、进行反向传播计算误差梯度时，求导涉及除法计算量少的多

3、不会出现梯度消失
### 五 Softmax函数
![](http://latex.codecogs.com/gif.latex?f(x_i)=\frac{exp(x_i)}{\sum_jexp(x_i)})

1、sigmoid函数针对二分类问题，softmax函数解决多分类问题

2、softmax函数基于多项式分布，sigmoid函数基于伯努利分布

3、softmax函数多分类时，类与类之间是互斥的，sigmoid函数输出的类之间并不是互斥的

![在这里插入图片描述](https://img-blog.csdnimg.cn/20190515192458845.jpg)
