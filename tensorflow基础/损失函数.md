满足条件：  
1）、非负性  
2）、当预测值接近真实值接近时，损失函数趋近于0  
（1）均方（平方）误差是一种较早的损失函数定义方法，比较直观，两个分布对应维度的差异性之和；  
（2）最大似然估计是从概率的角度出发的，其假设条件是“模型己定，参数未定”，例如，数据服从正
态分布，但是不知道其均值和万差， 即可求解出模型参数，使得p(y|x,θ)最大化；  
（3）最大化后验概率即使得慨率p(θ|x,y)最大化，具假设条件是参数θ有一个先验概率，实际上也等价于正则化项的最大似然估计，
它考虑了先验信息，通过对参数值的大小进行约束来防止“过拟合”；  
（4）交叉熵损失函数衡量两个分部p、q 的相似性。在给定集合上两个分布p和q的交叉摘定义如下：  
![](http://latex.codecogs.com/gif.latex?H\left(p,q\right)=E_{p}\left[-\log{q}\right]=H(p)+D_{kL}\left(p||q))  
![](http://latex.codecogs.com/gif.latex?H\left(p\right)=-\sum{p\left(x\right)\log{p\left(x\right)}})  
![](http://latex.codecogs.com/gif.latex?D_{KL}(p||q)=\sum{p\left(x\right)\log\dfrac{p\left(x\right)}{q\left(x\right)}})  
其中，H(p)是p的熵，D_{KL}(p||q)表示KL的散度（相对熵）。对于离散化的p和q：  
![](http://latex.codecogs.com/gif.latex?H\left(p,q\right)=\sum_x{p\left(x\right)\log{q\left(x\right)}}})  
TensorFlow中交叉熵的实现
```
cross_entropy = -tf.reduce_mean(y_*tf.log(tf.clip_by_value(y,1e-10,1.0)))
```
