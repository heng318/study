有四个概念graph，session，runtime，device。

前端系统：提供编程模型，负责构造计算图，支持多语言，其中Python提供的 API最为成熟。
后端系统：负责计算图的执行，使用C++实现，直接操作分布式的CPU/GPU环境，提供运行时环境，负责执行计算图。
<div align=center><img src="https://github.com/heng318/study/blob/master/images/%E7%B3%BB%E7%BB%9F%E6%9E%B6%E6%9E%84.png"/></div>

1、Client
Client是基于TensorFlow的编程接口，主要是用来构建计算图
系统中的4个基本组件，它们是系统分布式运行机制的核心
1. Client
   Client是前端系统的主要组成部分，它是一个支持多语言的编程环境。它提供基于计算图的编程模型，方便用户构造各种复杂的计算图，
   实现各种形式的模型设计。Client组件会创建Session，并通过序列化技术，发送图定义到Distributed Master组件。当Client触发Session运
   算的时候，Maser构建将要运行的子图。并根据设备情况，切割子图为多个分片。
2. Distributed Master
   Distributed Master用于构建子图；切割子图为多个分片，不同的子图分片运行在不同的设备上；Master还负责分发子图分片到Executor/Work端。
   Executor/Work在设备（CPUs，GPUs，etc.）上，调度执行子图操作，并负责向其它Worker发送和接收图操作的运行结果。在分布式的运行时环境中，
   Distributed Master根据Session.run的Fetching参数，从计算图中反向遍历，找到所依赖的最小子图。切割子图，把模型参数分组在参数服务器上，
   图计算操作分组在运算Worker上。Distributed Master会根据模型参数的分区情况进行切割边，在Task间插入发送和接收Tensor信息的通信节点。
   Distributed Master通过RegisterGraph方法发送子图分片给Task。Master通过RunGraph触发子图运算，Worker会使用GPU/CPU运算设备执行
   TensorFlow Kernel运算。在本节点的CPU和GPU之间，使用cudaMemcpyAsync传输数据；在本节点GPU和GPU之间，使用peer-to-peer DMA传输数据，
   避免通过CPU复制数据。TensorFlow使用gRPC（TCP）和RDMA （Converged Ethernet）技术
3. Worker Service
   Distributed Master和Worker Service只存在于分布式TensorFlow中。
4. Kernel Implement
   Kernel为TensorFlow中算法操作的具体实现，如卷积操作，激活操作等。
